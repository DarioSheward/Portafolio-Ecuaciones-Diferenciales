\documentclass[../portafolio.tex]{subfiles}


\begin{document}


\chapter{Variación del método de Newton-Raphson}
\label{g0_c5}
\hfill \textbf{Fecha de la actividad:} 17 de noviembre de 2024


\medskip


En este capítulo se revisa un método inspirado por Newton-Raphson para encontrar los ceros de una función, con especial utilidad para cero de multiplicidad mayor a 1. Se estima la tasa de convergencia de esta herramienta numérica y se aplica a con varias utilidades. Para realizar este análisis se utilizan series de Taylor mientras que para las representaciones gráficas se utilizan paquetes de Python.


\section*{Objetivos}
\begin{itemize}
\item Determinar la tasa de convergencia de la variación de Newton-Raphson. 
\item Aplicar la variación del método para maximizar la tasa de convergencia para cierta función.
\end{itemize}
Se presenta la siguiente variación del método de Newton-Raphson \eqref{g0_c5:eq:metodo}. 


\begin{equation} \label{g0_c5:eq:metodo}
x_{n+1}=x_n - \alpha \frac{f(x_n)}{f'(x_n)}
\end{equation}


Donde la función $f$ tiene una raíz $x=x^*$ de multiplicidad $m$, es decir, sus derivadas cumplen $f^{(p)}(x^* ) = 0$ para $p<m$. Sea $\alpha$ una constante arbitraria y $\lim_{n\rightarrow \infty} x_n = x^*$. 


\section{Tasa de convergencia del método}


Conociendo las anteriores condiciones, ahora podremos utilizarlas para determinar la tasa de convergencia.


Empezamos considerando que el error en el n-ésimo término es $\varepsilon_n = x^* - x_n$ y del siguiente término, $\varepsilon_{n+1} = x^* -x_{n+1}$. Reemplazando $x^*$ en la última expresión:
\begin{equation}
\varepsilon_{n+1} = \varepsilon_n + x_n - x_{n+1}
\end{equation}
Se reemplaza $x_{n+1}$ por como está definido el método \eqref{g0_c5:eq:metodo}:


\begin{align}
\varepsilon_{n+1} =& \quad \varepsilon_n + x_n - x_n + \alpha \frac{f(x_n)}{f'(x_n)}\\
\varepsilon_{n+1} =&  \quad \varepsilon_n + \alpha \frac{f(x_n)}{f'(x_n)} \label{g0_c5:impresion}
\end{align}


Cada término de $f$ se expande su serie de Taylor \citep{openstaxcalculo2} centrada en $x^*$, considerando que mientras $n$ aumente $x_n$ se acerca a esta raíz, y en el entorno $-\varepsilon_n$.


\begin{align*}
f(x_n)=&f(x^*)-f'(x^*)\varepsilon_n + \frac{f''(x^*)}{2!}\varepsilon^2_{n}+ \quad ...\\
f'(x_n)=&f'(x^*)-f''(x^*)\varepsilon_n + \frac{f'''(x^*)}{2!}\varepsilon^2_{n}+ \quad ...
\end{align*}


Recordar que $x^*$ es una raíz de multiplicidad $m$, por lo tanto:


\begin{align}
f(x_n)=&\frac{f^{(m)}(x^*)}{m!}\varepsilon^{m}_n - \frac{f^{(m+1)}(x^*)}{(m+1)!}\varepsilon^{m+1}_n+ \quad ...\\
f'(x_n)=&-\frac{f^{(m)}(x^*)}{(m-1)!}\varepsilon^{m-1}_n + \frac{f^{(m+1)}(x^*)}{m!}\varepsilon^{m}_n+ \quad ...
\end{align}


El dividirlos ahora no sería de mucha utilidad, así que acomodaremos los términos para poder usar la expansión de $\frac{1}{1-x}= \sum_{n=0}^\infty x^n$:


\begin{align}
\frac{f(x_n)}{f'(x_n)}\approx& \frac{\frac{f^{(m)}(x^*)}{m!}\varepsilon^{m}_n - \frac{f^{(m+1)}(x^*)}{(m+1)!}\varepsilon^{m+1}_n+ \quad ...}{-\frac{f^{(m)}(x^*)}{(m-1)!}\varepsilon^{m-1}_n + \frac{f^{(m+1)}(x^*)}{m!}\varepsilon^{m}_n+ \quad ...}\\
\frac{f(x_n)}{f'(x_n)}\approx &\frac{-\frac{\varepsilon_n}{m}+ \frac{f^{(m+1)}(x^*)}{m(m+1)f^{(m)}(x^*)}\varepsilon^{2}_n}{1  -  \frac{f^{(m+1)}(x^*)}{m f^{(m)}(x^*)}\varepsilon_n }\\
\frac{f(x_n)}{f'(x_n)}\approx& \left( -\frac{\varepsilon_n}{m}+ \frac{f^{(m+1)}(x^*)}{m(m+1)f^{(m)}(x^*)}\varepsilon^{2}_n \right) \left(1 +  \frac{f^{(m+1)}(x^*)}{m f^{(m)}(x^*)}\varepsilon_n \right)\\
\frac{f(x_n)}{f'(x_n)}\approx& \frac{-\varepsilon_n}{m} +\frac{f^{(m+1)}(x^*)}{m^2 f^{(m)}(x^*)}\varepsilon_n^{2}\left(1+\frac{1}{m}\right) + \left( \frac{f^{(m+1)}(x^*)}{m f^{(m)}(x^*)} \right)^2 \frac{\varepsilon_n^{3}}{m+1} \label{g0_c5:razooon}
\end{align}
%\frac{f(x_n)}{f'(x_n)}\approx &\frac{\frac{f^{(m)}(x^*)}{m!}\varepsilon^{m}_n  \frac{(m-1)!}{f^{(m)}(x^*)}- \frac{f^{(m+1)}(x^*)}{(m+1)!}\varepsilon^{m+1}_n \frac{(m-1)!}{f^{(m)}(x^*)}}{-\varepsilon^{m-1}_n + \frac{f^{(m+1)}(x^*)}{m!}\varepsilon^{m}_n \frac{(m-1)!}{f^{(m)}(x^*)}}\\
%\frac{f(x_n)}{f'(x_n)}\approx &\frac{\frac{\varepsilon^{m}_n}{m}- \frac{f^{(m+1)}(x^*)}{m(m+1)f^{(m)}(x^*)}\varepsilon^{m+1}_n}{-\varepsilon^{m-1}_n + \frac{f^{(m+1)}(x^*)}{m f^{(m)}(x^*)}\varepsilon^{m}_n }\\
Al reemplazar \eqref{g0_c5:razooon} en \eqref{g0_c5:impresion} los términos de las series que obviamos resultan insignificantes al ser de grados tan altos. 


\begin{align}
\varepsilon_{n+1} \approx&  \quad \varepsilon_n + \alpha \left( \frac{-\varepsilon_n}{m} +\frac{f^{(m+1)}(x^*)}{m^2 f^{(m)}(x^*)}\varepsilon_n^{2}\left(1+\frac{1}{m}\right) + \left( \frac{f^{(m+1)}(x^*)}{m f^{(m)}(x^*)} \right)^2 \frac{\varepsilon_n^{3}}{m+1} \right)
\end{align}
Los términos de grado mayor se utilizarán solo cuando los términos de grado 1 se cancelen.


\begin{align}
\varepsilon_{n+1} \approx&  \quad \varepsilon_n \left(1 - \frac{\alpha}{m} \right)
\end{align}


Para los casos donde $\alpha \neq m$ y $m\neq 0$, la tasa de convergencia del método \eqref{g0_c5:eq:metodo} será:


\begin{align}
\varepsilon_{n+1} =&  \quad \varepsilon_n \left(1 - \frac{\alpha}{m} \right) \label{g0_c5:tasaC}
\end{align}


Mostrando como el método converge linealmente.


\subsection{Convergencia cuadrática del método}


Ahora nos proponemos encontrar para que valor de $\alpha$ el método converge cuadráticamente \\
($\varepsilon_{n+1}\propto \varepsilon_n^2$).
Para esto buscamos los casos en los cuales $\varepsilon_n$ en \eqref{g0_c5:tasaC} tenga como factor un 0. Entonces, igualando este factor a 0:
\begin{align}
0=&1-\frac{\alpha}{m}\\
\alpha=& m
\end{align}


Sean todos los casos donde $\alpha=m$ de convergencia cuadrática, a una tasa de $1-\frac{\alpha}{m}$.


\subsection{Aplicación}
A continuación se aplicará el método de Newton-Raphson convencional \eqref{g0_c5:eq:metodo2} para encontrar las raíces de $f(x)=(x-1)^3$\footnote{Iniciaremos para todos estos procesos con $x_n\in  [-30,30] \backslash \{1\}$}. Claramente la raíz $x^*=1$ es de multiplicidad tres asi que podemos esperar que la tasa de convergencia sea dos tercios. Entonces, ahora se grafica el error absoluto de una aproximación y la cantidad de iteraciones que tomo llegar a ella \ref{g0_c5:fig:graf_c}.


\begin{equation} \label{g0_c5:eq:metodo2}
x_{n+1}=x_n -  \frac{f(x_n)}{f'(x_n)}
\end{equation}


\begin{figure}
\centering
\includegraphics[scale=0.75]{../img/2NR_6.png}
\caption{Aplicación de Newton-Raphson para $f(x)=(x-1)^3$, donde se expone el error absoluto contra la cantidad de iteraciones realizadas.} \label{g0_c5:fig:graf_c}
\end{figure}


Para comprobar los resultados de \eqref{g0_c5:eq:metodo} consideramos $m=3$ y $\alpha=1$, lo cual debería resultar en una tasa de convergencia de $\varepsilon_{n+1}/\varepsilon_n =2/3$. Esto es coherente con lo observable en \ref{g0_c5:fig:graf_c2}


\begin{figure}
\centering
\includegraphics[scale=0.75]{../img/3NR_6.png}
\caption{Comparación entre tasa de convergencia y los resultados numéricos.} \label{g0_c5:fig:graf_c2}
\end{figure}


Ahora repetiremos el proceso con $f(x)$, esta vez usando un valor de $\alpha$  que haga que el método converja cuadráticamente. Definimos $\alpha=3$ y repetimos el proceso de graficado en la figura \ref{g0_c5:fig:graf_c3}.


\begin{figure}
\centering
\includegraphics[scale=0.75]{../img/4NR_6.png}
\caption{Aplicación del método propuesto para convergencia cuadrática.} \label{g0_c5:fig:graf_c3}
\end{figure}




\subsection{Estimación de $\alpha$}
Con el objetivo de corregir el método original y aplicar nuestra variación, desconociendo la multiplicidad de la raíz, empezaremos realizando la cantidad mínima de iteraciones para calcular la relación \eqref{g0_c5:estimacion_m}, obteniendo una estimación de la multiplicidad de la raíz. 
\begin{equation}\label{g0_c5:estimacion_m}
\frac{\varepsilon_{n+1}}{\varepsilon_n^2} \approx \frac{1}{\tilde{m}}
\end{equation}


Con esta estimación ya se puede definir un $\alpha= \tilde{m}\approx \varepsilon_n^2/\varepsilon_{n+1}$. Este proceso se puede repetir, reemplazando $\alpha$ en cada iteración, llegando a mejores estimaciones de $m$.


\begin{equation}
\frac{\varepsilon_{n+1}}{\varepsilon_n^2} \approx \frac{\alpha}{\tilde{m}}
\end{equation}


En esta ocasión no se alcanzó a realizar un script de Python para este proceso, pero quede este propuesto para otra oportunidad.


\subparagraph{Script}
 Se inicia definiendo la expresión \eqref{g0_c5:eq:metodo} en Python. 
\begin{minted}{python}
def allnew(x,f=f,df=df,alpha=1):
    if x!=1:
        xn1=x- alpha*(f(x)/df(x))
        return xn1
    else:
        return 0
\end{minted}
Luego se definen constantes, arreglos, entre otras utilidades. Después se escoge un x como seed para el método, para cada iteración, se registra cada $\varepsilon_n=$\texttt{trai-root} y $n=$\texttt{iteraciones}.


\begin{minted}{python}
root=1
cantidad=1
tol=1e-10
errores=[]
ar_it=[]
itmax=100
xx=(np.linspace(-30,30))
for n in range(cantidad):
    x=np.any(xx)
    iteraciones=1
    trai= allnew(x)
    ar_it.append(iteraciones)
    errores.append(np.abs(trai-root))
    while  np.abs(f(trai)-root)>tol and iteraciones<itmax:
        trai=allnew(trai)
        iteraciones=1+ iteraciones
        ar_it.append(iteraciones)
        errores.append(np.abs(trai-root))
\end{minted}


Luego se graficaron las figuras \ref{g0_c5:fig:graf_c}, \ref{g0_c5:fig:graf_c2} y \ref{g0_c5:fig:graf_c3}.


\section{Análisis de resultados}
En general los resultados hablan por sí mismos. En la figura \ref{g0_c5:fig:graf_c} el error mínimo permanece constante tras las 90 iteraciones. Esto puede resultar de una limitación de la máquina, al menos dentro de la exploración de valores para la tolerancia utilizada, no hubo variación en que cantidad de iteraciones se estanca. Por lo que no se pueden obtener conclusiones de esto. \\
Las soluciones numéricas no resultaron del todo eficientes en cuanto memoria RAM se trata, sin embargo, como los métodos convergen rápidamente, no son necesarias tantas iteraciones.\\
La figura \ref{g0_c5:fig:graf_c3} presenta una singular distribución, esto se puede atribuir a un error computacional al aplicar el algoritmo que no he podido resolver ni identificar. Sin embargo, supongo que se puede deber al bajo error generado en las primeras iteraciones, generando luego rebotes periódicos.
\section*{Conclusiones}
Este ha sido un desafío muy completo, un trabajo interesante en la parte analítica, utilizando series de Taylor, que requiere de cuidados, y una parte práctica que solicita atención sobre qué datos conviene graficar y cuál es la información es realmente útil. Se presentaron dificultades para reconocer cuáles eran las limitaciones computacionales y las del método, precisamente a la hora de graficar. En general, me parece un trabajo satisfactorio a pesar de no poder lograr todos los objetivos propuestos.


\end{document}
